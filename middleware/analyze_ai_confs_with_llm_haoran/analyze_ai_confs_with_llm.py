import os
import json
from typing import Dict, List

import pandas as pd
import requests
import matplotlib.pyplot as plt

import numpy as np
import matplotlib
from matplotlib import font_manager
import time
from math import pi
import base64
from matplotlib.colors import LinearSegmentedColormap

# ============ Âü∫Êú¨ÈÖçÁΩÆ ============

# ‰Ω†ÁöÑ CSV ÊâÄÂú®ÁõÆÂΩïÔºàÊ†πÊçÆÂÆûÈôÖ‰øÆÊîπÔºâ
CSV_DIR = "data_dblp"

# ‰ºöËÆÆ‰ø°ÊÅØÔºàÂÅáËÆæÊñá‰ª∂ÂêçÊ†ºÂºè‰∏∫ {conf}_2021_2025_dblp.csvÔºâ
CONFERENCES = ["CVPR", "ECCV", "ICCV", "ICLR", "ICML", "MICCAI", "NeurIPS"]

# Âπ¥‰ªΩËåÉÂõ¥
YEARS = list(range(2021, 2026))

# ÂàÜÁ±ªÂàóÂêçÔºàÂíå‰Ω† CSV ÈáåÁöÑ‰∏ÄËá¥Ôºâ
CATEGORY_COLS = [
    "is_pretrain",
    "is_self_supervised",
    "is_segmentation",
    "is_detection",
    "is_classification",
    "is_generation",
    "is_reconstruction",
    "is_registration",
    "is_tracking",
    "is_pose",
    "is_video",
    "is_three_d",
    "is_multimodal",
    "is_fewshot",
    "is_semi_supervised",
    "is_domain_adaptation",
    "is_robustness",
    "is_graph",
    "is_rl",
    "is_transformer",
    "is_medical",
    "is_autonomous_driving",
    "is_nlp",
    "is_other",
]

CATEGORY_LABELS_EN: Dict[str, str] = {
    "pretrain": "Pretraining / Foundation",
    "self_supervised": "Self-supervised",
    "segmentation": "Segmentation",
    "detection": "Detection",
    "classification": "Classification / Recognition",
    "generation": "Generation / Diffusion / GAN",
    "reconstruction": "Reconstruction / SR",
    "registration": "Registration",
    "tracking": "Tracking / Trajectory",
    "pose": "Pose Estimation / Keypoints",
    "video": "Video / Temporal",
    "three_d": "3D / NeRF / Point Cloud",
    "multimodal": "Multimodal / V+L",
    "fewshot": "Few-shot / Zero-shot",
    "semi_supervised": "Semi / Weakly Supervised",
    "domain_adaptation": "Domain Adaptation / Transfer",
    "robustness": "Robustness / OOD / Privacy",
    "graph": "Graph / GNN",
    "rl": "Reinforcement Learning",
    "transformer": "Transformer / ViT",
    "medical": "Medical Imaging",
    "autonomous_driving": "Autonomous Driving / BEV",
    "nlp": "NLP / Language",
    "other": "Other / Long-tail",
}

FOCUS_CATS_FOR_CONF = [
    "pretrain",
    "segmentation",
    "detection",
    "generation",
    "multimodal",
    "three_d",
    "medical",
    "robustness",
]

RADAR_CATS = [
    "pretrain",
    "segmentation",
    "generation",
    "multimodal",
    "medical",
    "robustness",
]



# ============ LLM API ÈÖçÁΩÆÔºàÈÄöÁî® OpenAI-ÂÖºÂÆπÔºâ ============

LLM_API_BASE = os.getenv("LLM_API_BASE", "https://openrouter.ai/api/v1")
LLM_API_KEY = os.getenv("LLM_API_KEY", "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")
LLM_MODEL = os.getenv("LLM_MODEL", "tngtech/deepseek-r1t2-chimera:free")

# ============ ÂõæÂÉèÁîüÊàê API ÈÖçÁΩÆÔºàHugging FaceÔºâ ============

IMAGE_BACKEND = os.getenv("IMAGE_BACKEND", "hf")  # 'hf' Êàñ 'openrouter'

HF_API_TOKEN = os.getenv("HF_API_TOKEN", "")
HF_IMAGE_MODEL = os.getenv("HF_IMAGE_MODEL", "stabilityai/stable-diffusion-xl-base-1.0")

# ============ Êï∞ÊçÆÂä†ËΩΩ‰∏éÁªüËÆ° ============

def load_all_papers() -> pd.DataFrame:
    """ËØªÂèñÊâÄÊúâ‰ºöËÆÆÁöÑ CSVÔºåÂêàÂπ∂Êàê‰∏Ä‰∏™Â§ß DataFrame„ÄÇ"""
    dfs = []
    for conf in CONFERENCES:
        path = os.path.join(CSV_DIR, f"{conf}_2021_2025_dblp.csv")
        if not os.path.exists(path):
            print(f"[WARN] CSV ‰∏çÂ≠òÂú®ÔºåË∑≥Ëøá: {path}")
            continue
        print(f"[INFO] ËØªÂèñ {path}")
        df = pd.read_csv(path)
        dfs.append(df)
    if not dfs:
        raise RuntimeError("Ê≤°ÊúâËØªÂà∞‰ªª‰Ωï CSVÔºåËØ∑Ê£ÄÊü• CSV_DIR ÂíåÊñá‰ª∂Âêç„ÄÇ")
    df_all = pd.concat(dfs, ignore_index=True)
    return df_all


def build_yearly_stats(df: pd.DataFrame) -> Dict[int, dict]:
    """
    ÂØπÊØè‰∏ÄÂπ¥ÂÅöËÅöÂêàÁªüËÆ°Ôºö
    - total_papers: ÊÄªËÆ∫ÊñáÊï∞
    - category_stats: ÂêÑÁ±ªÂà´ÁöÑ count / ratio / example_titles
    - top_categories: ÊåâÊï∞ÈáèÊéíÂ∫èÁöÑÂâç‰∏â‰∏™ÊñπÂêë
    """
    stats: Dict[int, dict] = {}

    for year in YEARS:
        df_y = df[df["year_target"] == year]
        if df_y.empty:
            print(f"[WARN] {year} Âπ¥Ê≤°Êúâ‰ªª‰ΩïËÆ∫ÊñáÔºåË∑≥Ëøá„ÄÇ")
            continue

        total = int(len(df_y))
        cat_stats: Dict[str, dict] = {}

        for col in CATEGORY_COLS:
            if col not in df_y.columns:
                continue
            cat_key = col.replace("is_", "")  # ‰æãÂ¶Ç is_pretrain -> pretrain
            count = int(df_y[col].sum())
            ratio = float(count) / total if total > 0 else 0.0

            if count > 0:
                titles = (
                    df_y[df_y[col] == 1]["title"]
                    .dropna()
                    .astype(str)
                    .unique()
                    .tolist()
                )
                example_titles = titles[:8]  # ÊØèÁ±ªÊúÄÂ§ö 8 ÁØá‰ª£Ë°®‰æã
            else:
                example_titles = []

            cat_stats[cat_key] = {
                "count": count,
                "ratio": ratio,
                "example_titles": example_titles,
            }

        sorted_cats = sorted(
            cat_stats.items(), key=lambda kv: kv[1]["count"], reverse=True
        )
        top3 = [
            {
                "category": cat,
                "count": data["count"],
                "ratio": data["ratio"],
            }
            for cat, data in sorted_cats[:3]
            if data["count"] > 0
        ]

        stats[year] = {
            "year": year,
            "total_papers": total,
            "category_stats": cat_stats,
            "top_categories": top3,
        }

        if top3:
            print(
                f"[INFO] {year} Âπ¥ÔºöÊÄªËÆ∫Êñá {total} ÁØáÔºå"
                f"top1 = {top3[0]['category']} ({top3[0]['count']} ÁØá)"
            )
        else:
            print(f"[INFO] {year} Âπ¥ÔºöÊÄªËÆ∫Êñá {total} ÁØá")

    return stats


def build_trend_data(year_stats: Dict[int, dict]) -> dict:
    """
    ÊûÑÂª∫ 2021‚Äì2025 ÁöÑÊó∂Èó¥Â∫èÂàóÊï∞ÊçÆÔºåÁªôÂ§ßÊ®°ÂûãÂÅöÊï¥‰ΩìË∂ãÂäøÂàÜÊûêÔºö
    {
      "years": [2021, 2022, ...],
      "total_papers": {2021: xxx, ...},
      "by_category": {
          "pretrain": [2021_count, 2022_count, ...],
          ...
      }
    }
    """
    years_sorted = sorted(year_stats.keys())
    trend = {
        "years": years_sorted,
        "total_papers": {y: year_stats[y]["total_papers"] for y in years_sorted},
        "by_category": {},
    }

    for col in CATEGORY_COLS:
        cat_key = col.replace("is_", "")
        yearly_counts: List[int] = []
        for y in years_sorted:
            cat_info = year_stats[y]["category_stats"].get(cat_key, {})
            yearly_counts.append(int(cat_info.get("count", 0)))
        trend["by_category"][cat_key] = yearly_counts

    return trend


# ============ Prompt ÊûÑÈÄ† ============

def make_year_prompt(year: int, year_payload: dict) -> str:
    """
    ‰∏∫Êüê‰∏ÄÂπ¥ÁöÑÁªüËÆ°Êï∞ÊçÆÁîüÊàê‚ÄúÂ∞èÁ∫¢‰π¶È£éÊ†º‚ÄùÊñáÊ°àÁöÑÊèêÁ§∫ËØç„ÄÇ
    ËßÜËßíÔºöÁ´ôÂú® 2025 Âπ¥ÁöÑ PapergentÔºåÂõûÂ§¥Â§çÁõòËøô‰∏ÄÂπ¥„ÄÇ
    """
    year_data_json = json.dumps(year_payload, ensure_ascii=False, indent=2)

    prompt = f"""
‰Ω†ÊòØ‰∏ÄÂêçÂú®Â∞èÁ∫¢‰π¶ÂàÜ‰∫´ AI / CV ÊñπÂêëÂÜÖÂÆπÁöÑÂçö‰∏ªÔºåÂõ∫ÂÆö‰∫∫ËÆæÊòØÔºö

> Â§ßÂÆ∂Â•ΩÔºåÊàëÊòØÊù•Ëá™‰∏≠ÁßëÂ§ßËãèÁ†îÈô¢ Miracle Lab ÁöÑ PapergentÔºå‰∏ÄÂè™‰∏ìÈó®Â∏ÆÂ§ßÂÆ∂„ÄåÁúãËÆ∫Êñá„ÄÅÈÅøÂùëÈÄâÈ¢ò„ÄçÁöÑÊô∫ËÉΩÂ∞èÁ∫∏‰∫∫ üìÑü§ñ

Áé∞Âú®ÊòØ **2025 Âπ¥**Ôºå‰Ω†Êâã‰∏äÊúâ 2021‚Äì2025 Âπ¥Â§ö‰∏™ AI È°∂‰ºöÔºàCVPR, ICCV, ECCV, ICLR, ICML, MICCAI, NeurIPSÔºâÁöÑËÆ∫ÊñáÁªüËÆ°Êï∞ÊçÆÔºå
Â∑≤ÁªèÊåâ‰∏ªÈ¢òÔºàpretrain, segmentation, generation, multimodal, medical Á≠âÔºâËÅöÂêàÂ•Ω‰∫Ü„ÄÇ

‰∏ãÈù¢ÊòØ **{year} Âπ¥** ÁöÑÁªüËÆ° JSONÔºàÂè™Áªô‰Ω†ÁúãÔºå‰∏çË¶ÅÂéüÂ∞Å‰∏çÂä®Ë¥¥Âá∫Êù•ÔºâÔºö

{year_data_json}

Â≠óÊÆµËØ¥ÊòéÔºö
- total_papers: Ëøô‰∏ÄÂπ¥ÊâÄÊúâËÆ∫ÊñáÊÄªÊï∞
- category_stats: ÈîÆÊòØÁ±ªÂà´ÂêçÔºàÂ¶Ç "pretrain"ÔºâÔºåÂåÖÂê´ count„ÄÅratio„ÄÅexample_titles Á≠â
- top_categories: ÊåâËÆ∫ÊñáÊï∞ÈáèÊéíÂ∫èÁöÑÂâç‰∏âÂ§ßÊñπÂêë

‚ö†Ô∏è ÁâπÂà´ËßÑÂàôÔºö
- Â¶ÇÊûú top_categories ÈáåÁöÑÁ¨¨‰∏ÄÂêçÁ±ªÂà´ÊòØ "other"ÔºåËØ¥ÊòéËøôÊòØ‰∏Ä‰∏™‚ÄúÊùÇÈ°πÊ°∂‚ÄùÔºåËØ∑‰∏çË¶ÅÊääÂÆÉÂΩìÊàê‰∏ªËßíÂàÜÊûêÔºõ
  ‰ºòÂÖàËÆ≤‰πãÂêéÈÇ£‰∫õÊõ¥ÂÖ∑‰ΩìÁöÑÊñπÂêëÔºàpretrain, segmentation, generation, multimodal, medical Á≠âÔºâ„ÄÇ
- ‚Äúother‚Äù ÂèØ‰ª•Âú®ÊñáÊ°à‰∏≠È°∫Â∏¶‰∏ÄÊèêÔºåÊØîÂ¶Ç‚ÄúËøòÊúâ‰∏ÄÂ§ßÂ†ÜÈïøÂ∞æÂ∞èÊñπÂêë‰∏¢Âú® other Èáå‚ÄùÔºå‰ΩÜ‰∏çË¶ÅÊîæÂú® C ‰Ωç„ÄÇ

Áé∞Âú®ËØ∑‰Ω†Ê†πÊçÆËøô‰∫õÊï∞ÊçÆÔºåÂÜôÂá∫ **2 Êù°ÈÄÇÂêàÂèëÂú®Â∞èÁ∫¢‰π¶‰∏äÁöÑÁü≠ÊñáÊ°à**ÔºåË¶ÅÊ±ÇÔºö

### Êï¥‰ΩìÈ£éÊ†º
- ËØ≠Ë®ÄÔºö**‰∏≠Êñá‰∏∫‰∏ª**ÔºåÂ∞ëÈáèÂ§πÂ∏¶Ëã±ÊñáÊúØËØ≠
- ËØ≠Ê∞îÔºöËΩªÊùæ„ÄÅÊúâË∂£„ÄÅÂÉè Papergent Âú®Ë∑üÁ≤â‰∏ùËÅäÂ§©Ôºå‰∏çË¶ÅËÆ∫ÊñáÂè£Âêª
- ËßÜËßíÔºö**ÊòéÁ°ÆÊòØ 2025 Âπ¥Âú®ÂõûÂ§¥ËÅä {year} Ëøô‰∏ÄÂπ¥ÁöÑÊÉÖÂÜµ**ÔºåÂèØ‰ª•ÂÅ∂Â∞îÂØπÊØîÂêéÈù¢Âá†Âπ¥ÔºàÊØîÂ¶Ç‚ÄúÂêéÊù• 2023‚Äì2025 ËØÅÊòé‰∫ÜËøô‰∏ÄÊ≥¢Ë∂ãÂäø‚ÄùÔºâ„ÄÇ
- ÈïøÂ∫¶ÔºöÊØèÊù°ÊéßÂà∂Âú® **200‚Äì400 Â≠ó** Â∑¶Âè≥
- ÁªìÊûÑÔºö‰∏çË¶ÅÁî®„Äå1. 2. 3.„ÄçËøôÁßçÂ≠¶ÊúØÂ§ßÁ∫≤Ôºå‰∏çË¶ÅÁî® Markdown Ê†áÈ¢ò (#)Ôºõ
  ÂÖÅËÆ∏ÈÄÇÂΩìÊç¢Ë°åÂàÜÊÆµÔºåÁî®ÁÆÄÂçïÁöÑÂàóË°®Á¨¶Âè∑ÔºàÊØîÂ¶Ç„Äå¬∑„ÄçÊàñ„Äå-„ÄçÔºâ„ÄÇ

### ÊØèÊù°ÊñáÊ°àÈÉΩÂøÖÈ°ªÂåÖÂê´ÁöÑÂÜÖÂÆπ

1. **ÂºÄÂ§¥‰∫∫ËÆæ + Âê∏ÁùõÊ†áÈ¢òÔºà2025 ËßÜËßíÔºâ**  
   - Á¨¨‰∏ÄË°åÁî®‰∏ÄÂè•Ê†áÈ¢òÂºèÁöÑËØùÔºåÊØîÂ¶ÇÔºö  
     - ‚ÄúÂõûÂà∞ {year}ÔºöËøôÂπ¥È°∂‰ºö AI Âà∞Â∫ïÂú®Âç∑Âï•Ôºüüî•ÔºàÊù•Ëá™ 2025 ÁöÑÂõûÁúãÔºâ‚Äù  
     - ‚ÄúÁ´ôÂú® 2025 ÂõûÂ§¥Áúã {year}ÔºåÂéüÊù•Ëøô‰∏ÄÂπ¥Êó©Â∞±ÂüãÂ•Ω‰∫ÜÂêéÈù¢Âá†Âπ¥ÁöÑ‰ºèÁ¨î üìà‚Äù  
   - ‰∏ã‰∏ÄË°åÁî®‰∏ÄÂè•ËØùÂè£Êí≠ÂºèËá™Êàë‰ªãÁªç + Êó∂Èó¥ËßÜËßíÔºö  
     ‚ÄúÂ§ßÂÆ∂Â•ΩÔºåÊàëÊòØ‰∏≠ÁßëÂ§ßËãèÁ†îÈô¢ Miracle Lab ÁöÑ PapergentÔºåÁé∞Âú®Âú® 2025 Âπ¥ÔºåÂ∏¶‰Ω†ÂõûÂ§¥ÁúãÁúã {year} ÈÇ£Âπ¥ AI È°∂‰ºöÈÉΩÂú®Âøô‰∫õ‰ªÄ‰πàÔΩû‚Äù

2. **Êï∞ÊçÆÊù•Ê∫êËØ¥ÊòéÔºà‰∏ÄÂÆöË¶ÅÊèê DBLPÔºâ**  
   Áî® 1‚Äì2 Âè•‰∫§‰ª£Ê∏ÖÊ•öÔºö  
   - Êú¨ÊñáÊâÄÊúâÁªüËÆ°ÈÉΩÂü∫‰∫é **DBLP ÁöÑËÆ∫ÊñáÊî∂ÂΩïÊï∞ÊçÆ**Ôºå  
   - ÂèØËÉΩ‰∏çÊòØ 100% ÂÆåÊï¥ÔºàÊúâÂ∞ëÊï∞ËÆ∫ÊñáÊ≤°Êî∂ÂΩïÔºâÔºå  
   - ‰ΩÜÂØπ CVPR/ICLR/NeurIPS Ëøô‰∫õ‰∏ªÊµÅÈ°∂‰ºöÊù•ËØ¥Ôºå**Â∑≤ÁªèË¶ÜÁõñÁªùÂ§ßÂ§öÊï∞ËÆ∫ÊñáÔºåÁî®Êù•ÁúãÂ§ßÁõòË∂ãÂäøÊòØË∂≥Â§üÁöÑ**„ÄÇ  
   ËØ≠Ê∞îÂèØ‰ª•Á®çÂæÆÂπΩÈªò‰∏ÄÁÇπÔºåÊØîÂ¶Ç‚ÄúÂ∞±ÂΩìÊòØ AI È°∂‰ºöÂúàÁöÑÊ∞îË±°Âè∞ËßÇÊµãÊï∞ÊçÆ‚Äù„ÄÇ

3. **{year} Âπ¥ÁöÑÁÉ≠ÁÇπÊñπÂêëÔºà‰ªé 2025 ÂõûÂ§¥ÁúãÔºâ**  
   - ÈáçÁÇπËÆ≤ 2‚Äì3 ‰∏™ËÆ∫ÊñáÈáèÊúÄÂ§ö„ÄÅ‰∏î‰∏çÊòØ "other" ÁöÑÊñπÂêëÔºö  
     - Á≤óÁï•Êèê‰∏Ä‰∏ãÊï∞ÈáèÁ∫ßÂíåÂç†ÊØîÔºàÁî®‚ÄúÂ∑Æ‰∏çÂ§öÂç†Âà∞ÊÄªÈáèÁöÑ 1/5 Â∑¶Âè≥‚Äù„ÄÅ‚ÄúÂú®ÂΩìÂπ¥Â∑≤ÁªèÊòØÂ¶•Â¶•ÁöÑ‰∏ªÊµÅ‚ÄùÁ≠âÂÆöÊÄßÊèèËø∞ÔºâÔºå  
     - Áî®ÂæàÈÄö‰øóÁöÑËØùÊ¶ÇÊã¨Ëøô‰∫õÊñπÂêëÂú®Âπ≤ÂòõÔºàÊ†πÊçÆÁ±ªÂà´ÂêçÂíå example_titles ÂêàÁêÜÊ¶ÇÊã¨Ôºâ„ÄÇ  
   - ÂèØ‰ª•È°∫Â∏¶Â∏¶‰∏ÄÁÇπ‚ÄúÂêéËßÜÈïú‚ÄùËßÜËßíÔºö  
     - ÊØîÂ¶Ç‚Äú‰ªé 2025 ÂõûÂ§¥ÁúãÔºåËøô‰∏ÄÂπ¥Âú® pretraining ‰∏äÈì∫ÁöÑË∑ØÔºåÂêéÊù•Áõ¥Êé•ÂñÇÈ•±‰∫ÜÂêéÈù¢Âá†‰ª£Â§ßÊ®°Âûã‚ÄùÔºõ  
     - ÊàñËÄÖ‚ÄúÂΩìÊó∂Â§ßÂÆ∂Ê≤°ËßâÂæóÊúâÂ§öÁÅ´ÁöÑÊñπÂêëÔºåÂêéÊù•Âú® 2023‚Äì2024 ÂèòÊàêÈªëÈ©¨‚Äù„ÄÇ

4. **Â∞ëÈáèËÆ∫Êñá‰æãÂ≠êÔºà‰ø°ÊÅØÈáèÔºå‰ΩÜ‰∏çÂï∞Âó¶Ôºâ**  
   - ÂèØ‰ª•‰ªé example_titles ÈáåÊåë 1‚Äì3 ‰∏™Ê†áÈ¢òÔºö  
     Áî®„Äå„ÄäÊ†áÈ¢ò„Äã+ ‰∏ÄÂè•ÂêêÊßΩ/Ëß£ËØª„ÄçÁöÑÂΩ¢ÂºèÁªôÂá∫ÔºåÊØîÂ¶ÇÔºö  
     ‚Äú„ÄäXXXX„ÄãËøôÁßçÂ∞±ÊòØÂú®ÂÅöÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢Ôºå‰∏ªÊâì‰∏Ä‰∏™‚ÄòËÆ©Ê®°ÂûãÈ°∫‰æøËØªÊáÇÂõæ+Êñá‚ÄôÔºåÂà∞ 2025 Áúã‰ªçÁÑ∂ÊòØÂæàÂÖ∏ÂûãÁöÑ‰∏ÄÁ±ªÂ∑•‰Ωú„ÄÇ‚Äù  
   - ‰∏çË¶ÅÂàóÂæàÈïøÁöÑ paper listÔºå‰øùÊåÅËΩªÈáèÁ∫ß„ÄÇ

5. **Â∞èÂ∞èÈÄâÈ¢òÊÑüÂèó + ‰∫íÂä®ÈóÆÈ¢ò + Ê†áÁ≠æÔºà2025 ËßÜËßíÔºâ**  
   - Áî® 1 ÊÆµËØùÔºå‰ªé 2025 ÁöÑËßÜËßíÁªôÊ≠£Âú®ÈÄâÈ¢ò/ÂõûÈ°æÂ±•ÂéÜÁöÑ‰∫∫‰∏ÄÁÇπÊÑüËßâÔºåÊØîÂ¶ÇÔºö  
     ‚ÄúÂ¶ÇÊûú‰Ω†Âú® {year} ÈÇ£Âπ¥ÈÄâ‰∫Ü pretraining / multimodalÔºåÁé∞Âú®Â§ßÊ¶ÇÁéáÂ∑≤ÁªèË∫∫Âú®Êµ™Â∞ñ‰∏äÔºõ  
      Â¶ÇÊûúÂΩìÊó∂ÈÄâ‰∫ÜÊüê‰∫õÈïøÂ∞æÂ∞èÊñπÂêëÔºåÁé∞Âú®‰πüÊúâÂèØËÉΩÂèòÊàê‚ÄòÂÜ∑Èó®ÂÆùËóèËÇ°‚Äô„ÄÇ‚Äù  
   - ÊúÄÂêéÊäõ‰∏Ä‰∏™ÈóÆÈ¢òËÆ©Â§ßÂÆ∂ËØÑËÆ∫Âå∫‰∫íÂä®Ôºå‰æãÂ¶ÇÔºö  
     ‚ÄúÂ¶ÇÊûúËÉΩÁ©øË∂äÂõû {year} ÁªôÂΩìÂπ¥ÁöÑËá™Â∑±‰∏ÄÂè•ÈÄâÈ¢òÂª∫ËÆÆÔºå‰Ω†‰ºöËØ¥Âï•Ôºü‚Äù  
   - Âä†‰∏ä 3‚Äì6 ‰∏™ hashtagÔºàÁî® # Á¨¶Âè∑ÔºâÔºå‰æãÂ¶ÇÔºö  
     #AIÁ†îÁ©∂ #È°∂‰ºöÂ§çÁõò #CVPR #NeurIPS #ÁßëÁ†îÈÄâÈ¢ò #Papergent

### Emoji ‰ΩøÁî®
- ÊØèÊù°ÊñáÊ°à‰ΩøÁî® 3‚Äì8 ‰∏™ emojiÔºöüìàüìâüìäü§ñüß†üî•‚ú®üß™üöóü©ª Á≠âÈÉΩÂèØ‰ª•
- ‰∏çË¶ÅÊØèÂè•ËØùÈÉΩÂ°û emojiÔºå‰øùÊåÅËá™ÁÑ∂ÁÇπÁºÄ

### ËæìÂá∫Ê†ºÂºè
- ‰∏ÄÊ¨°ÊÄßËæìÂá∫ 2 Êù°ÊñáÊ°à
- ‰∏§Êù°ÊñáÊ°à‰πãÈó¥Áî®‰∏ÄË°å `---` ÂàÜÈöî
- ‰∏çË¶ÅÂÜçËæìÂá∫ JSONÔºå‰∏çË¶ÅËß£Èáä‰Ω†ÊòØÂ¶Ç‰ΩïÂàÜÊûêÁöÑÔºåÂè™ÁªôÊàëÂèØ‰ª•Áõ¥Êé•Â§çÂà∂Âà∞Â∞èÁ∫¢‰π¶ÈáåÁöÑ‰∏≠ÊñáÊñáÊú¨

ËØ∑Ê†πÊçÆ‰ª•‰∏äË¶ÅÊ±ÇÔºåÁªìÂêà {year} Âπ¥ÁöÑÊï∞ÊçÆÔºå**‰ª• 2025 Âπ¥ÁöÑËßÜËßíÂÜôÂá∫Ëøô‰∏§Êù°Âπ¥Â∫¶ÂõûÈ°æÊñáÊ°à**„ÄÇ
"""
    return prompt




def make_trend_prompt(trend_payload: dict) -> str:
    """
    ÁîüÊàê‰∏ÄÁØá‚Äú‰ªé 2025 ËßÜËßíÁúã 2021‚Äì2025 AI È°∂‰ºöË∂ãÂäø + ÂØπ 2026 È¢ÑÊµã‚ÄùÁöÑÂ∞èÁ∫¢‰π¶È£éÊ†ºÊñáÊ°à„ÄÇ
    """
    trend_json = json.dumps(trend_payload, ensure_ascii=False, indent=2)

    prompt = f"""
‰Ω†ÊòØ‰∏ÄÂêçÂú®Â∞èÁ∫¢‰π¶ÂÅö AI ÁßëÊôÆ&Ë∂ãÂäøÂàÜÊûêÁöÑÂçö‰∏ªÔºåÂõ∫ÂÆö‰∫∫ËÆæÊòØÔºö

> Â§ßÂÆ∂Â•ΩÔºåÊàëÊòØÊù•Ëá™‰∏≠ÁßëÂ§ßËãèÁ†îÈô¢ Miracle Lab ÁöÑ PapergentÔºå‰∏ÄÂè™‰∏ìÈó®Â∏Æ‰Ω†ÁúãÈ°∂‰ºö„ÄÅÁõòË∂ãÂäøÁöÑÊô∫ËÉΩÂ∞èÁ∫∏‰∫∫ üìÑü§ñ

**Áé∞Âú®ÊòØ 2025 Âπ¥**Ôºå‰Ω†Êâã‰∏äÊúâ 2021‚Äì2025 Âπ¥Â§ö‰∏™È°∂‰ºöÔºàCVPR, ICCV, ECCV, ICLR, ICML, MICCAI, NeurIPSÔºâÁöÑËÆ∫ÊñáÁªüËÆ°Êó∂Èó¥Â∫èÂàóÔºå
ÊåâÁ±ªÂà´Ôºàpretrain, segmentation, generation, multimodal, medical Á≠âÔºâËÅöÂêàÊàê‰∫Ü JSONÔºö

{trend_json}

ËØ¥ÊòéÔºö
- years: [2021, 2022, ...]
- total_papers: ÊØèÂπ¥ÊÄªËÆ∫ÊñáÊï∞
- by_category: ÊØè‰∏™Á±ªÂà´ÔºåÂØπÂ∫î 2021‚Äì2025 ÊØèÂπ¥ÁöÑËÆ∫ÊñáÊï∞ÈáèÂàóË°®

‚ö†Ô∏è ÁâπÂà´Ê≥®ÊÑèÔºö
- ÊâÄÊúâÁªüËÆ°ÈÉΩÂü∫‰∫é **DBLP ÁöÑËÆ∫ÊñáÊî∂ÂΩïÊï∞ÊçÆ**Ôºå  
  ÂèØËÉΩ‰ºöÊºèÊéâÂ∞ëÊï∞ËÆ∫ÊñáÊàñÊúâ‰∏™Âà´Êî∂ÂΩïÂÅèÂ∑ÆÔºå‰ΩÜÂØπËøô‰∫õÈ°∂‰ºöÊù•ËØ¥Â∑≤ÁªèË¶ÜÁõñÁªùÂ§ßÂ§öÊï∞ËÆ∫ÊñáÔºå**ÈùûÂ∏∏ÈÄÇÂêàÁúãÊï¥‰ΩìË∂ãÂäø**„ÄÇ  
  ÂèØ‰ª•Âú®ÊñáÊ°àÈáåÁî®ËΩªÊùæÁöÑÊñπÂºèÊèêÂà∞Ëøô‰∏ÄÁÇπÔºàÊØîÂ¶Ç‚ÄúÂ∞±ÂΩìÊòØÈ°∂‰ºöÂúàÁöÑÊ∞îË±°Âè∞Êï∞ÊçÆ‚ÄùÔºâ„ÄÇ
- Â¶ÇÊûúÊüê‰∏™ÊñπÂêëÁöÑ key ÊòØ "other"ÔºåÂÆÉÂè™ÊòØ‰∏Ä‰∏™‚ÄúÊùÇÈ°πÊ°∂‚ÄùÔºåÂú®ÂàÜÊûê‰∏ªË¶ÅË∂ãÂäøÊó∂‰∏çË¶ÅÊääÂÆÉÂΩì‰∏ªËßíÔºåÂèØ‰ª•È°∫Â∏¶ÊèêÂà∞‰ΩÜ‰∏çÁî®ÈáçÁÇπÂ±ïÂºÄ„ÄÇ

ËØ∑‰Ω†ÂÜôÂá∫ **1 Êù°ÈÄÇÂêàÂèëÂú®Â∞èÁ∫¢‰π¶‰∏äÁöÑË∂ãÂäøÊñáÊ°à**ÔºåÂÜÖÂÆπÊòØÔºö
‚ÄúÁ´ôÂú® 2025 Âπ¥ÔºåÂõûÂ§¥Áúã 2021‚Äì2025 È°∂‰ºöÊñπÂêëÊºîÂåñ + ÂØπ 2026 ÁöÑÈ¢ÑÊµã‚Äù„ÄÇ

### È£éÊ†ºË¶ÅÊ±Ç
- ‰∏≠Êñá‰∏∫‰∏ªÔºåÂ∞ëÈáèËã±ÊñáÊúØËØ≠
- ËØ≠Ê∞îËΩªÊùæ„ÄÅÂ•ΩÁé©„ÄÅÂÉè Papergent Âú®Ë∑üËØªËÄÖËÅäÂ§©Ôºå‰∏çË¶ÅÂ≠¶ÊúØ review Âè£Âêª
- ÊòéÁ°ÆÁöÑÊó∂Èó¥ËßÜËßíÔºö**2025 Âπ¥ÁöÑÁé∞Âú®ÔºåÂõûÈ°æËøáÂéª‰∫îÂπ¥ÔºåÂπ∂ÂæÄ 2026 ÂæÄÂâçÁúã**„ÄÇ
- ÊÄªÈïøÂ∫¶ÊéßÂà∂Âú® **400‚Äì700 Â≠ó**ÔºåÂèØ‰ª•ÂàÜ 4‚Äì7 ÊÆµÁü≠ËØù
- ‰∏çË¶ÅÁî® Markdown Ê†áÈ¢òÔºà#ÔºâÔºå‰πü‰∏çË¶Å 1.2.3 ËøôÁßçÂ≠¶ÊúØÂ§ßÁ∫≤

### ÂÜÖÂÆπË¶ÅÁÇπÔºàÊï¥Êù°ÊñáÊ°àÂ§ßËá¥ÁªìÊûÑÔºâ

1. **ÂºÄÂ§¥‰∫∫ËÆæ + ÊÄª‰ΩìË∂ãÂäø‰∏ÄÂè•ËØùÔºà2025 ËßÜËßíÔºâ**  
   - Á¨¨‰∏ÄË°åÂèØ‰ª•ÊòØÁ±ª‰ººÔºö‚Äú‰∫îÂπ¥È°∂‰ºöËÆ∫ÊñáË∂ãÂäø‰∏ÄÂõæÁúãÂÆå üìà Ë∞ÅÂú®ÁãÇÈ£ôÔºåË∞ÅÂú®ÈÄÄÂú∫ÔºüÔºàÊù•Ëá™ 2025 ÁöÑÂõûÁúãÔºâ‚Äù  
   - ‰∏ã‰∏ÄÂè•Áî®Á¨¨‰∏Ä‰∫∫Áß∞‰ªãÁªçËá™Â∑±ÂíåÊó∂Èó¥ËßÜËßíÔºö  
     ‚ÄúÂ§ßÂÆ∂Â•ΩÔºåÊàëÊòØ‰∏≠ÁßëÂ§ßËãèÁ†îÈô¢ Miracle Lab ÁöÑ PapergentÔºåÁé∞Âú®Á´ôÂú® 2025 Âπ¥ÔºåÂ∏Æ‰Ω†Êää 2021‚Äì2025 ËøôÊ≥¢ AI È°∂‰ºöÁÉ≠ÊΩÆÊï¥‰ΩìËøá‰∏ÄÈÅç„ÄÇ‚Äù

2. **Êï∞ÊçÆÊù•Ê∫êÁöÑËØ¥ÊòéÔºàDBLPÔºâ**  
   - Áî® 1‚Äì2 Âè•‰∫§‰ª£Ôºö  
     ‚ÄúÊâÄÊúâÊï∞ÊçÆÊù•Ëá™ DBLP ÁöÑËÆ∫ÊñáÊî∂ÂΩïÁªüËÆ°Ôºå‰∏çÊòØÂÆòÊñπÊéíË°åÊ¶úÔºå‰ΩÜÂØπ CVPR/ICLR/NeurIPS Á≠âÈ°∂‰ºöÊù•ËØ¥Â∑≤ÁªèË¶ÜÁõñ‰∫ÜÁªùÂ§ßÂ§öÊï∞ËÆ∫ÊñáÔºåÁî®Êù•ÁúãÂ§ßÁõòË∂ãÂäøÈùûÂ∏∏Èù†Ë∞±„ÄÇ‚Äù  
   - ÂèØ‰ª•Âä†‰∏ÄÂè•ËΩªÊùæÁöÑÁ±ªÊØîÔºåÊØîÂ¶Ç‚ÄúÂ∞±ÂΩìÊòØ AI È°∂‰ºöÂúàËøáÂéª‰∫îÂπ¥ÁöÑÂ§©Ê∞îËÆ∞ÂΩï‚Äù„ÄÇ

3. **Ê†∏ÂøÉÊñπÂêëÁöÑË∂ãÂäøÈÄüÂÜôÔºà‰ªé 2025 ÂõûÂ§¥ÁúãÔºâ**  
   - Êåë 4‚Äì6 ‰∏™‰Ω†ËÆ§‰∏∫ÊúÄÂÖ≥ÈîÆÁöÑÊñπÂêëÔºàÊØîÂ¶Ç pretrain/self_supervised„ÄÅgeneration„ÄÅmultimodal„ÄÅmedical„ÄÅrobustness„ÄÅ3D Á≠âÔºâÔºå  
     Áî®ÈùûÂ∏∏Âè£ËØ≠ÁöÑÊñπÂºèÊ¶ÇÊã¨ÂÆÉ‰ª¨Âú® 2021‚Äì2025 ÁöÑËµ∞ÂäøÔºö  
     - Ë∞ÅÊòØ‰∏ÄË∑ØÁãÇÈ£ôÁöÑ‚ÄúÈ°∂ÊµÅÊñπÂêë‚ÄùÔºàÂèØ‰ª•ÊèèËø∞Êàê‚Äú‰ªéÂ∞è‰ºóÂèòÊàêÊâÄÊúâ‰∫∫ÈÉΩÂæóÂç∑‰∏ÄÂç∑‚ÄùÔºâ„ÄÅ  
     - Ë∞ÅÊòØ‰∏ÄÁõ¥Á®≥Á®≥Âú®Á∫øÁöÑ‚ÄúËÄÅÁâåÊâìÂ∑•‰∫∫‚Äù„ÄÅ  
     - Ë∞ÅÊòØÊúÄËøë‰∏§Âπ¥Á™ÅÁÑ∂ËπøËµ∑Êù•ÁöÑ‚ÄúÈªëÈ©¨‚Äù„ÄÇ  
   - ‰∏çÈúÄË¶ÅÁªôÂá∫Á≤æÁ°ÆÊï∞Â≠óÔºå‰ΩÜË¶Å‰ΩìÁé∞Áõ∏ÂØπÊ∂®Ë∑åÂíåÈáèÁ∫ßÊÑüÔºåÂèØ‰ª•Áî®‚Äú‰ªéÊØèÂπ¥Âá†ÂçÅÁØáÊ∂®Âà∞‰∏äÁôæÁØá‚ÄùËøôÁßçÂÆöÊÄßÊèèËø∞„ÄÇ  
   - Â¶ÇÊûúÂêàÁêÜÔºåÂèØ‰ª•È°∫Â∏¶ÁÇπÂá∫ few-shot / robustness / domain adaptation ËøôÁßç‚ÄúËôΩÁÑ∂‰∏çÊòØÊúÄÂ§ßÁõòÔºå‰ΩÜÂæàÂÖ≥ÈîÆ‚ÄùÁöÑÊîØÁ∫øÊñπÂêë„ÄÇ

4. **2026 Âèä‰πãÂêé 3 Âπ¥ÁöÑËΩªÈáèÈ¢ÑÊµãÔºà2025 ÂæÄÂâçÁúãÔºâ**  
   - Áî® 1‚Äì2 ÊÆµËØ¥Ôºö  
     - Âì™ 2‚Äì3 ‰∏™ÊñπÂêëÂ§ßÊ¶ÇÁéáÁªßÁª≠Ë∂ÖÈ´òÁÉ≠Â∫¶ÔºàÊØîÂ¶Ç pretraining„ÄÅÂ§öÊ®°ÊÄÅ„ÄÅÂ§ßÊ®°ÂûãÁõ∏ÂÖ≥ÔºâÔºå  
     - Âì™‰∫õÊñπÂêë‰ºöËøõÂÖ•‚ÄúËÆ∫ÊñáÂæàÂ§ö‰ΩÜÂàõÊÑèÂºÄÂßãÂêåË¥®Âåñ‚ÄùÁöÑÂπ≥Âè∞ÊúüÔºå  
     - Papergent Ëá™Â∑±ÊúÄÁúãÂ•ΩÁöÑ 1‚Äì2 ‰∏™‰∫§ÂèâÊñπÂêëÔºà‰æãÂ¶Ç‚ÄúÂ§öÊ®°ÊÄÅ + ÂåªÁñó‚Äù„ÄÅ‚Äú3D + Ëá™Âä®È©æÈ©∂‚Äù„ÄÅ‚Äúrobustness + ÂÆâÂÖ®‚ÄùÁ≠âÔºâÔºå  
       ‰ª• 2025 ÁöÑËßÜËßíËß£Èáä‰∏Ä‰∏ã‰∏∫‰ªÄ‰πàÁúãÂ•Ω„ÄÇ  
   - ËØ≠Ê∞î‰øùÊåÅÊòØÈ¢ÑÊµãËÄå‰∏çÊòØÊñ≠Ë®ÄÔºåÂèØ‰ª•Áî®‚ÄúÊàëËá™Â∑±ÁöÑÊÑüËßâÊòØ‚Ä¶‚Äù„ÄÅ‚ÄúÂ§ßÊ¶ÇÁéá‰ºö‚Ä¶‚ÄùÊù•Âº±ÂåñÁªùÂØπÊÑü„ÄÇ

5. **ÁªôÈÄâÈ¢ò/Êã©‰∏öÁöÑËØªËÄÖ‰∏ÄÁÇπÁõ¥ËßÇÂª∫ËÆÆ + ‰∫íÂä®**  
   - Áî® 1 Â∞èÊÆµÂëäËØâÂ§ßÂÆ∂Ôºö  
     - ‚ÄúÂ¶ÇÊûú‰Ω†ÂñúÊ¨¢Âç∑Âü∫Á°ÄÁêÜËÆ∫ / ÁÆóÊ≥ïÔºåÂèØ‰ª•ÂæÄÂì™Âá†‰∏™ÊñπÂêëÁúãÔºõ  
        Â¶ÇÊûúÊõ¥ÊÉ≥ÂÅöËêΩÂú∞Â∫îÁî®ÔºåÂèØ‰ª•ÂÖ≥Ê≥®Âì™‰∫õËµõÈÅìÔºõ  
        ÊÉ≥ÂÅöÂÜ∑Èó®ÂÆùËóèÔºå‰πüÂèØ‰ª•‰ªéÂì™‰∫õÂ¢ûÈïøÂø´‰ΩÜ‰ΩìÈáè‰∏çÂ§ßÁöÑÊñπÂêëÈáåÈÄâ„ÄÇ‚Äù  
   - ÊúÄÂêéÊäõ‰∏Ä‰∏™ÈóÆÈ¢òÔºö‚Äú‰ªéËøô‰∫îÂπ¥ÁöÑË∂ãÂäøÈáåÔºå‰Ω†ËßâÂæóËá™Â∑±Â∫îËØ•Á´ôÂú®Âì™‰∏™ËµõÈÅì‰∏äÔºü‰Ω†Áé∞Âú®ÁöÑÊñπÂêëÔºåÊòØÈ°∫È£éËΩ¶ËøòÊòØÈÄÜÈ£éÁõòÔºü‚Äù  
   - Âä†‰∏ä 4‚Äì8 ‰∏™ hashtagÔºå‰æãÂ¶ÇÔºö  
     #AIË∂ãÂäø #È°∂‰ºöËÆ∫Êñá #CVPR #NeurIPS #ÁßëÁ†îÊñπÂêë #2026È¢ÑÊµã #Papergent

### Emoji ‰ΩøÁî®
- ÂÖ®Êñá‰ΩøÁî®Â§ßÁ∫¶ 5‚Äì12 ‰∏™ emojiÔºåÂàÜÂ∏ÉÂú®‰∏çÂêåÂè•Â≠êÈáå
- Â∏∏Áî®ÁöÑÂèØ‰ª•ÊòØÔºöüìàüìâüìäü§ñüß†üî•‚ú®üöóü©ªüåäüåà Á≠â

### ËæìÂá∫Ê†ºÂºè
- Âè™ËæìÂá∫‰∏ÄÊù°ÂÆåÊï¥ÊñáÊ°à
- Áõ¥Êé•ÁªôÊàëÂèØ‰ª•Â§çÂà∂Âà∞Â∞èÁ∫¢‰π¶ÈáåÁöÑÊñáÊú¨Ôºå‰∏çË¶ÅËß£Èáä‰Ω†ÁöÑÂàÜÊûêËøáÁ®ãÔºå‰πü‰∏çË¶ÅÂ∏¶ JSON

ËØ∑Ê†πÊçÆ‰∏äËø∞Ë¶ÅÊ±ÇÂíåÁªôÂÆöÁöÑ 2021‚Äì2025 ÁªüËÆ°Êï∞ÊçÆÔºå**‰ª• 2025 Âπ¥ÁöÑËßÜËßíÁîüÊàêËøôÊù°Ë∂ãÂäø+È¢ÑÊµãÊñáÊ°à**„ÄÇ
"""
    return prompt




# ============ ÈÄöÁî® LLM Ë∞ÉÁî® ============

def call_llm(prompt: str, max_retries: int = 5) -> str:
    """Ë∞ÉÁî® OpenRouter Chat APIÔºåÂ∏¶ 429 ÈôêÊµÅÈáçËØï„ÄÇ"""
    if not LLM_API_KEY:
        raise RuntimeError(
            "Êú™ËÆæÁΩÆ LLM_API_KEYÔºåËØ∑ÂÖàÂú® shell ‰∏≠ export LLM_API_KEY='‰Ω†ÁöÑOpenRouter key'„ÄÇ"
        )

    url = LLM_API_BASE.rstrip("/") + "/chat/completions"
    headers = {
        "Authorization": f"Bearer {LLM_API_KEY}",
        "Content-Type": "application/json",
        # Ëøô‰∏§‰∏™ header ÂèØÈÄâ
        # "HTTP-Referer": "https://your-site-or-github",
        # "X-Title": "ai-conference-trend-analyzer",
    }

    payload = {
        "model": LLM_MODEL,
        "messages": [
            {
                "role": "system",
                "content": "‰Ω†ÊòØ‰∏Ä‰∏™ÊìÖÈïøÈòÖËØªÂ§ßËßÑÊ®°ËÆ∫ÊñáÁªüËÆ°Êï∞ÊçÆÂπ∂Êí∞ÂÜôÂ≠¶ÊúØË∂ãÂäøÊä•ÂëäÁöÑÂä©Êâã„ÄÇ",
            },
            {
                "role": "user",
                "content": prompt,
            },
        ],
        "temperature": 0.7,
        "max_tokens": 4096,
    }

    for attempt in range(1, max_retries + 1):
        resp = requests.post(url, headers=headers, json=payload, timeout=120)

        # 429ÔºöÈôêÊµÅÔºåÊâìÂç∞ÊèêÁ§∫ÔºåÁÑ∂ÂêéÈáçËØï
        if resp.status_code == 429:
            print(f"\n[LLM WARN] Êî∂Âà∞ 429 ÈôêÊµÅÔºàÁ¨¨ {attempt}/{max_retries} Ê¨°Â∞ùËØïÔºâ")
            try:
                print("[LLM WARN BODY]:", resp.text[:500])
            except Exception:
                pass

            if attempt == max_retries:
                resp.raise_for_status()

            # ÁÆÄÂçïÊåáÊï∞ÂõûÈÄÄÔºåÈÅøÂÖç‰∏ÄÁõ¥ÊâìÁàÜÂêå‰∏Ä‰∏™Ê®°Âûã
            sleep_secs = 5 * attempt
            print(f"[LLM WARN] ÊöÇÂÅú {sleep_secs} ÁßíÂêéÈáçËØïÂêå‰∏ÄÊ¨°ËØ∑Ê±Ç...")
            time.sleep(sleep_secs)
            continue

        if resp.status_code != 200:
            print("\n[LLM ERROR] status =", resp.status_code)
            print("[LLM ERROR BODY]:")
            print(resp.text[:1000])
            resp.raise_for_status()

        data = resp.json()
        return data["choices"][0]["message"]["content"]

    raise RuntimeError("LLM ËØ∑Ê±ÇÂ§öÊ¨°ÈáçËØïÂêé‰ªçÁÑ∂Â§±Ë¥•")




def save_text(path: str, content: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)


# ============ ÂèØËßÜÂåñÔºàÂÅöÂæóÊõ¥ÈÄÇÂêàÁ§æÂ™íÔºâ ============

def plot_year_category_bars(
    year_stats: Dict[int, dict], out_dir: str = "figs/year_bars"
):
    """
    For each year, draw a horizontal bar chart of top-N categories (English labels only),
    with a more modern palette and denser layout for social-media style.
    """
    os.makedirs(out_dir, exist_ok=True)

    for year, info in year_stats.items():
        cat_stats = info["category_stats"]
        items = [(cat, s["count"]) for cat, s in cat_stats.items() if s["count"] > 0]
        if not items:
            continue

        # sort by count desc, take top 12
        items.sort(key=lambda x: x[1], reverse=True)
        top_items = items[:12]
        cats = [c for c, _ in top_items]
        counts = np.array([n for _, n in top_items], dtype=int)

        labels = [CATEGORY_LABELS_EN.get(c, c) for c in cats]
        y_pos = np.arange(len(counts))

        # nice multi-hue palette
        base_cmap = plt.cm.get_cmap("viridis")
        colors = base_cmap(np.linspace(0.15, 0.9, len(counts)))

        fig = plt.figure(figsize=(10, 6))
        ax = fig.add_subplot(111)

        bars = ax.barh(
            y_pos,
            counts,
            color=colors,
            height=0.6,
            edgecolor="#ffffff",
            linewidth=0.8,
        )

        ax.set_yticks(y_pos)
        ax.set_yticklabels(labels)
        ax.set_xlabel("Number of Papers")
        ax.set_title(f"{year} ¬∑ Top 12 Research Directions", pad=14)

        # only vertical gridlines for a cleaner look
        ax.xaxis.grid(True)
        ax.yaxis.grid(False)

        # remove top/right spines, keep bottom/left very light
        for spine in ["top", "right"]:
            ax.spines[spine].set_visible(False)
        for spine in ["left", "bottom"]:
            ax.spines[spine].set_color("#DDDDDD")

        max_count = counts.max()
        for bar, cnt in zip(bars, counts):
            x = bar.get_width()
            y = bar.get_y() + bar.get_height() / 2
            ax.text(
                x + max_count * 0.01,
                y,
                f"{int(cnt)}",
                va="center",
                ha="left",
                fontsize=9,
                color="#333333",
            )

        fig.tight_layout()
        out_path = os.path.join(out_dir, f"year_{year}_top_categories.png")
        fig.savefig(out_path, dpi=320)
        plt.close(fig)
        print(f"[OK] Saved figure: {out_path}")




def plot_trend_lines(trend_data: dict, out_dir: str = "figs/trends"):
    """
    Plot time-series trends (2021‚Äì2025) for several key directions:
    1) multi-line chart
    2) stacked area chart
    All labels are in English to avoid encoding issues.
    """
    os.makedirs(out_dir, exist_ok=True)
    years = trend_data["years"]

    focus_cats = [
        "pretrain",
        "generation",
        "multimodal",
        "three_d",
        "video",
        "medical",
        "robustness",
    ]

    # ===== multi-line chart =====
    plt.figure(figsize=(10, 6))
    colors = plt.cm.Set2(np.linspace(0, 1, len(focus_cats)))

    for i, cat in enumerate(focus_cats):
        if cat not in trend_data["by_category"]:
            continue
        counts = trend_data["by_category"][cat]
        label = CATEGORY_LABELS_EN.get(cat, cat)
        plt.plot(
            years,
            counts,
            marker="o",
            linewidth=2.3,
            color=colors[i],
            label=label,
        )

    plt.title("Key Directions: Paper Count Trends (2021‚Äì2025)", fontsize=14, pad=12)
    plt.xlabel("Year", fontsize=11)
    plt.ylabel("Number of Papers", fontsize=11)
    plt.xticks(years)
    plt.legend(loc="upper left", fontsize=9, frameon=True, framealpha=0.9)
    plt.tight_layout()

    out_path = os.path.join(out_dir, "trend_focus_multi_lines.png")
    plt.savefig(out_path, dpi=260)
    plt.close()
    print(f"[OK] Saved trend line figure: {out_path}")

    # ===== stacked area chart (robust version) =====
    valid_labels = []
    series_list = []
    for cat in focus_cats:
        if cat not in trend_data["by_category"]:
            continue
        series_list.append(trend_data["by_category"][cat])
        valid_labels.append(CATEGORY_LABELS_EN.get(cat, cat))

    if series_list:
        lengths = [len(s) for s in series_list]
        T = min(min(lengths), len(years))
        if T == 0:
            print("[WARN] No data for stacked area in plot_trend_lines.")
            return

        if len(set(lengths)) != 1 or T != len(years):
            print(
                f"[WARN] trend_lines series length mismatch: "
                f"min={min(lengths)}, max={max(lengths)}, years_len={len(years)}; "
                f"using first {T} points."
            )

        years_plot = years[:T]
        data = np.row_stack(
            [np.array(s[:T], dtype=float) for s in series_list]
        )

        colors_area = plt.cm.Set3(np.linspace(0, 1, data.shape[0]))

        plt.figure(figsize=(10, 6))
        plt.stackplot(years_plot, data, labels=valid_labels, colors=colors_area, alpha=0.9)
        plt.title("Key Directions: Stacked Area (2021‚Äì2025)", fontsize=14, pad=12)
        plt.xlabel("Year", fontsize=11)
        plt.ylabel("Number of Papers (stacked)", fontsize=11)
        plt.xticks(years_plot)
        plt.legend(loc="upper left", fontsize=9, frameon=True, framealpha=0.9)
        plt.tight_layout()

        out_path = os.path.join(out_dir, "trend_focus_stack_area.png")
        plt.savefig(out_path, dpi=260)
        plt.close()
        print(f"[OK] Saved stacked area figure: {out_path}")


def save_cover_image_prompt(path: str = "figs/cover_image_prompt.txt") -> None:
    """
    1. ‰øùÂ≠ò‰∏ÄÊÆµÁªô‰∫∫ÁúãÁöÑ‰∏≠ÊñáÂ∞ÅÈù¢ÊèíÁîª promptÔºõ
    2. Â¶ÇÊûúÈÖçÁΩÆ‰∫ÜÂõæÂÉèÁîüÊàê APIÔºåÂàôÔºö
       - IMAGE_BACKEND=hf  -> Ë∞É Hugging Face router + SDXL
       - IMAGE_BACKEND=openrouter -> Ë∞É OpenRouterÔºà‰øùÊåÅÂéüÈÄªËæëÔºâ
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)

    # ===== ‰∫∫Á±ªÈòÖËØªÁâàÔºà‰∏≠ÊñáÔºåÂÜôÊñá‰ª∂Áî®Ôºâ=====
    human_prompt = """
Áîª‰∏ÄÂº†Á´ñÁâà 4:5 ÊØî‰æãÁöÑÊº´ÁîªÈ£éÊèíÁîªÔºåÁî®‰∫éÂ∞èÁ∫¢‰π¶Â∞ÅÈù¢„ÄÇ

ÁîªÈù¢Ë¶ÅÁ¥†Ôºö
- ‰∏≠Â§ÆËßíËâ≤Ôºö‰∏Ä‰∏™Êãü‰∫∫ÂåñÁöÑÂ∞èÁ∫∏‰∫∫Êú∫Âô®‰∫∫ PapergentÔºåË°®ÊÉÖËÅ™ÊòéÂèàÊúâÁÇπÂëÜËêåÔºåÊà¥ÁùÄÁúºÈïúÊàñÂ§¥Êà¥ÂºèËÄ≥Êú∫ÔºåÊâãÈáåÊä±ÁùÄ‰∏ÄÊëûËÆ∫Êñá„ÄÇ
- ËÆ∫ÊñáÂ∞ÅÈù¢‰∏äÂèØ‰ª•ÈöêÁ∫¶ÁúãÂà∞Âá†‰∏™Ëã±ÊñáÂçïËØçÔºöCVPR„ÄÅICCV„ÄÅECCV„ÄÅICLR„ÄÅICML„ÄÅMICCAI„ÄÅNeurIPS„ÄÇ
- ËÉåÊôØÔºöÊúâ‰∏Ä‰∫õÂçäÈÄèÊòéÁöÑÁªüËÆ°ÂõæË°®ÂÖÉÁ¥†ÔºåÊØîÂ¶ÇÊäòÁ∫øÂõæ„ÄÅÊü±Áä∂Âõæ„ÄÅÂ†ÜÂè†Èù¢ÁßØÂõæ„ÄÅÁÉ≠ÂäõÂõæÔºåÈ¢úËâ≤ÊüîÂíåÔºå‰∏çË¶ÅÂ§™Êä¢Êàè„ÄÇ
- Êï¥‰ΩìÈ£éÊ†ºÔºöÁÆÄÊ¥Å„ÄÅÊòé‰∫Æ„ÄÅÁßëÊäÄÊÑü‰ΩÜ‰∏çÂÜ∞ÂÜ∑ÔºåÂÅè‰∫åÊ¨°ÂÖÉ / ÊâÅÂπ≥ÊèíÁîªÈ£éÔºåÈÄÇÂêàÂ∞èÁ∫¢‰π¶Â∞ÅÈù¢„ÄÇ
- ÈÖçËâ≤ÔºöÂÅèÁ≤âÁ¥´ + ËìùËâ≤Á≥ªÔºåÂèØ‰ª•ÁÇπÁºÄÂ∞ëÈáèÈªÑËâ≤ÊàñÁªøËâ≤ÔºåËê•ÈÄ†ËΩªÊùæ‰ΩÜ‰∏ì‰∏öÁöÑÊÑüËßâ„ÄÇ
- ÊûÑÂõæÔºö‰∏äÊñπÂíå‰∏ãÊñπÂ∞ΩÈáèÁïôÂá∫‰∏Ä‰∫õÂπ≤ÂáÄÁ©∫Èó¥ÔºåÊñπ‰æøÂêéÊúüÂè†Âä†‰∏≠ÊñáÊ†áÈ¢òÊñáÂ≠ó„ÄÇ

Á¶ÅÊ≠¢Ôºö
- ‰∏çË¶ÅÁîªÂá∫ÂÖ∑‰ΩìÁöÑÁúüÂÆû‰∫∫Áâ©„ÄÅlogo ÊàñÊïèÊÑüÂÜÖÂÆπ„ÄÇ
- ‰∏çË¶ÅÂá∫Áé∞Ëøá‰∫éÂ§çÊùÇÁöÑËÉåÊôØÁªÜËäÇÔºå‰øùÊåÅÊï¥‰ΩìÁÆÄÊ¥Å„ÄÇ

ËØ∑ËæìÂá∫‰∏ÄÂº†È´òÂàÜËæ®Áéá„ÄÅÁ∫øÊù°Ê∏ÖÊô∞ÁöÑÊèíÁîª„ÄÇ
""".strip()

    with open(path, "w", encoding="utf-8") as f:
        f.write(human_prompt)
    print(f"[OK] Saved (human) cover image prompt to {path}")

    # ===== Ê®°Âûã‰∏ìÁî®ÁâàÔºàËã±ÊñáÂÖ≥ÈîÆËØçÔºåÁªô SDXL / ÂÖ∂‰ªñ text2img ÂêÉÔºâ=====
    model_prompt = (
        "vertical 4:5 manga style illustration, clean and simple, "
        "a small chibi paper robot character named Papergent, "
        "wearing big round glasses and large over-ear headphones, "
        "sitting on a pile of research papers and books, holding an open paper. "
        "around the character there are many flying papers, sticky notes and tabs, "
        "with clear text on the covers such as CVPR, ICCV, ECCV, ICLR, ICML, MICCAI, NeurIPS. "
        "in the background there are a few soft, semi-transparent data visuals: "
        "a line chart, a bar chart, a stacked area chart and a heatmap window. "
        "overall look is cute, modern and nerdy, comic / anime style, "
        "with balanced pleasant colors (not dominated by a single color), "
        "high resolution, sharp clean line art, designed as a social media cover image."
    )


    negative_prompt = (
        "no realistic human, no ancient painting, no traditional hanfu clothes, "
        "no classical chinese man, no landscape, no mountains, no pine trees, "
        "no dark muddy colors, no calligraphy, no oil painting style"
    )

    backend = os.getenv("IMAGE_BACKEND", IMAGE_BACKEND).lower()

    # ====================== ÊñπÊ°à AÔºöHugging Face Router ======================
    if backend == "hf":
        if not HF_API_TOKEN:
            print("[IMG WARN] HF_API_TOKEN not set; only saved text prompt.")
            return

        model_id = HF_IMAGE_MODEL
        url = f"https://router.huggingface.co/hf-inference/models/{model_id}"
        headers = {
            "Authorization": f"Bearer {HF_API_TOKEN}",
            "Content-Type": "application/json",
        }
        # 4:5 ÊØî‰æãÔºåÁ®çÂæÆÈ´ò‰∏ÄÁÇπÂàÜËæ®ÁéáÔºåÈÄÇÂêàÂ∞èÁ∫¢‰π¶Â∞ÅÈù¢
        payload = {
            "inputs": model_prompt,
            "parameters": {
                "negative_prompt": negative_prompt,
                "width": 896,     # 4:5 -> 896x1120
                "height": 1120,
                "num_inference_steps": 30,
                "guidance_scale": 7.5,
            },
            "options": {"wait_for_model": True},
        }

        print(f"[IMG] Requesting cover image from HuggingFace router: {model_id}")
        try:
            resp = requests.post(url, headers=headers, json=payload, timeout=300)
            print(f"[IMG] HTTP status: {resp.status_code}")

            if resp.status_code != 200:
                print("[IMG ERROR] body:")
                print(resp.text[:800])
                if resp.status_code == 403:
                    print(
                        "[IMG HINT] 403 ForbiddenÔºöÂΩìÂâç HF_API_TOKEN Ê≤°ÊúâË∞ÉÁî® Inference ÁöÑÊùÉÈôê„ÄÇ\n"
                        "ËØ∑Âà∞ https://huggingface.co/settings/tokens ÈáçÊñ∞ÂàõÂª∫‰∏Ä‰∏™Â∏¶ Inference/API ÊùÉÈôêÁöÑ tokenÔºå\n"
                        "ÁÑ∂ÂêéÂú®ÊúçÂä°Âô®‰∏äÔºö export HF_API_TOKEN='hf_xxx' ÂÜçË∑ë‰∏ÄÊ¨°„ÄÇ"
                    )
                resp.raise_for_status()

            img_bytes = resp.content
            out_img_path = os.path.join(os.path.dirname(path), "cover_image.png")
            with open(out_img_path, "wb") as f_img:
                f_img.write(img_bytes)

            print(f"[OK] Saved cover image to {out_img_path}")

        except Exception as e:
            print(f"[IMG ERROR] Failed to generate cover image via HF: {e}")

        return

    # ====================== ÊñπÊ°à BÔºöOpenRouterÔºà‰øùÊåÅÂéüÈÄªËæëÔºâ ======================
    if backend == "openrouter":
        base = os.getenv("LLM_API_BASE", "https://openrouter.ai/api/v1")
        key = os.getenv("LLM_API_KEY", "")
        image_model = os.getenv("LLM_IMAGE_MODEL", "").strip()

        if not key or not image_model:
            print(
                "[WARN] LLM_API_KEY Êàñ LLM_IMAGE_MODEL Êú™ËÆæÁΩÆÔºå"
                "Âè™‰øùÂ≠ò‰∫ÜÊñáÂ≠ó promptÔºåÊ≤°ÊúâÁîüÊàêÂ∞ÅÈù¢ÂõæÁâá„ÄÇ"
            )
            return

        url = base.rstrip("/") + "/chat/completions"
        headers = {
            "Authorization": f"Bearer {key}",
            "Content-Type": "application/json",
        }
        payload = {
            "model": image_model,
            "messages": [{"role": "user", "content": model_prompt}],
            "modalities": ["image", "text"],
            "image_config": {"aspect_ratio": "4:5"},
            "max_output_images": 1,
        }

        print(f"[IMG] Requesting cover image from OpenRouter model: {image_model}")
        try:
            resp = requests.post(url, headers=headers, json=payload, timeout=120)
            print(f"[IMG] HTTP status: {resp.status_code}")
            if resp.status_code != 200:
                print("[IMG ERROR] body:")
                print(resp.text[:800])
                resp.raise_for_status()

            data = resp.json()
            choice = data["choices"][0]["message"]
            images = choice.get("images")
            if not images:
                print("[IMG WARN] API ËøîÂõû‰∏≠Ê≤°Êúâ images Â≠óÊÆµÔºåÂèØËÉΩÊòØÊ®°Âûã‰∏çÊîØÊåÅÂõæÁâáÁîüÊàêÊàñÂèÇÊï∞‰∏çÊ≠£Á°Æ„ÄÇ")
                return

            img_info = images[0]["image_url"]["url"]
            if not img_info.startswith("data:image"):
                print("[IMG WARN] Êî∂Âà∞ÁöÑ image_url ‰∏çÊòØ data URLÔºåÊöÇÊó∂‰∏çËß£ÊûêÔºö", img_info[:80])
                return

            header, b64_data = img_info.split(",", 1)
            img_bytes = base64.b64decode(b64_data)

            out_img_path = os.path.join(os.path.dirname(path), "cover_image.png")
            with open(out_img_path, "wb") as f_img:
                f_img.write(img_bytes)

            print(f"[OK] Saved cover image to {out_img_path}")

        except Exception as e:
            print(f"[IMG ERROR] Failed to generate cover image via OpenRouter: {e}")

        return

    # ====================== Êú™ËØÜÂà´ backend ======================
    print(
        f"[IMG INFO] IMAGE_BACKEND='{backend}' Êú™ËØÜÂà´Ôºå"
        "ÁõÆÂâçÂè™ÊîØÊåÅ 'hf' Êàñ 'openrouter'ÔºåÊú¨Ê¨°Âè™‰øùÂ≠ò‰∫ÜÊñáÂ≠ó prompt„ÄÇ"
    )



def plot_category_share_stacked_area(
    trend_data: dict,
    top_k: int = 6,
    out_dir: str = "figs/stacked_share",
):
    """
    Áîª 2021‚Äì2025 ÊñπÂêëÂç†ÊØîÊºîÂåñÔºàstacked areaÔºâ„ÄÇ

    ‰øÆÊ≠£ÁâàÔºö
    - ÂÖàÊ£ÄÊü•ÊØè‰∏™Á±ªÂà´ÁöÑÊó∂Èó¥Â∫èÂàóÈïøÂ∫¶ÔºåÂèñÊúÄÁü≠ÈïøÂ∫¶ TÔºõ
    - years ‰πüÊà™Âà∞Ââç T Âπ¥Ôºõ
    - ÊØè‰∏™Á±ªÂà´ÈÉΩÊà™Âà∞Ââç T ‰∏™ÁÇπÔºå‰øùËØÅ row_stack Êó∂ÊâÄÊúâË°åÈïøÂ∫¶‰∏ÄËá¥Ôºõ
    - main_cats Âè™‰ªéÈùû "other" Á±ªÂà´ÈáåÈÄâÔºå"other" Ë°åÁªü‰∏ÄË°®Á§∫‚ÄúÈô§‰∫Ü top_k ‰ª•Â§ñÁöÑÊâÄÊúâÁ±ªÂà´‚Äù„ÄÇ
    """
    os.makedirs(out_dir, exist_ok=True)

    years = list(trend_data["years"])
    by_cat = trend_data["by_category"]

    # ÂêÑÁ±ªÂà´ÁöÑÊó∂Èó¥Â∫èÂàóÈïøÂ∫¶
    lengths = [len(v) for v in by_cat.values() if hasattr(v, "__len__")]
    if not lengths:
        print("[WARN] No category data for stacked area plot.")
        return

    # Áªü‰∏Ä‰ΩøÁî®ÁöÑÊó∂Èó¥ÈïøÂ∫¶ T
    T = min(min(lengths), len(years))
    if T == 0:
        print("[WARN] Empty time axis for stacked area plot.")
        return

    if len(set(lengths)) != 1 or T != len(years):
        print(
            f"[WARN] category series length mismatch: "
            f"min={min(lengths)}, max={max(lengths)}, years_len={len(years)}; "
            f"using first {T} points for all."
        )

    # Êà™Êñ≠Âπ¥‰ªΩ & ÊØè‰∏™Á±ªÂà´ÁöÑËÆ°Êï∞Â∫èÂàó
    years = years[:T]
    by_cat_trimmed = {
        cat: list(counts)[:T]
        for cat, counts in by_cat.items()
        if hasattr(counts, "__len__")
    }

    # Âè™Âú®Èùû "other" ÈáåÈÄâ top_k ‰∏ªËßí
    total_per_cat = {
        cat: int(sum(counts))
        for cat, counts in by_cat_trimmed.items()
        if cat != "other" and sum(counts) > 0
    }
    if not total_per_cat:
        print("[WARN] No non-zero main categories for stacked area plot.")
        return

    sorted_cats = sorted(total_per_cat.items(), key=lambda kv: kv[1], reverse=True)
    main_cats = [c for c, _ in sorted_cats[:top_k]]
    other_cats = [c for c in by_cat_trimmed.keys() if c not in main_cats]

    print("[STACKED] main categories:", main_cats)
    print("[STACKED] other bucket contains:", other_cats)

    num_years = len(years)
    K = len(main_cats) + 1  # ÂÜçÂä†‰∏Ä‰∏™‚Äúother‚Äù
    data = np.zeros((K, num_years), dtype=float)

    # ÈÄêÂπ¥ËÆ°ÁÆóÂç†ÊØî
    for t in range(num_years):
        total_tags = sum(by_cat_trimmed[c][t] for c in by_cat_trimmed)
        if total_tags == 0:
            continue

        # ‰∏ªÁ±ªÂç†ÊØî
        for idx, c in enumerate(main_cats):
            val = by_cat_trimmed[c][t]
            data[idx, t] = val / total_tags

        # ÂÖ∂‰ªñÁ±ªÂà´Âç†ÊØî
        other_val = sum(by_cat_trimmed[c][t] for c in other_cats)
        data[-1, t] = other_val / total_tags

    labels = main_cats + ["other"]
    label_names = [
        CATEGORY_LABELS_EN.get(
            c,
            "Other / Long-tail" if c == "other" else c,
        )
        for c in labels
    ]

    print("[STACKED] data shape:", data.shape)  # (K, T)

    # -------- ËøôÈáåÊòØÊñ∞ÁöÑÈÖçËâ≤ÊñπÊ°à --------
    # Ââç top_k ‰∏™‰∏ªÁ±ªÔºö‰ΩøÁî®‰∏Ä‰∏™ÂÅèËìùÁªø„ÄÅÊØîËæÉÊüîÂíåÁöÑ colormap
    num_main = len(main_cats)
    cmap_main = plt.cm.get_cmap("PuBuGn")
    main_colors = cmap_main(np.linspace(0.35, 0.9, num_main))

    # ÊúÄÂêéÁöÑ "other"ÔºöÁî®ÂæàÊµÖÁöÑ‰∏≠ÊÄßÁÅ∞ÔºåÈôç‰ΩéÂ≠òÂú®ÊÑüÔºå‰∏çÂÜçÊòØÂ§ßÂùó‰∫ÆÈªÑËâ≤
    other_color = np.array([[0.90, 0.90, 0.93, 1.0]])  # RGBA
    colors = np.vstack([main_colors, other_color])
    # ---------------------------------

    plt.figure(figsize=(10, 6))
    plt.stackplot(years, data, labels=label_names, colors=colors, alpha=0.95)
    plt.title("Research Direction Share Over Time", fontsize=14, pad=12)
    plt.xlabel("Year", fontsize=11)
    plt.ylabel("Share of Category Tags", fontsize=11)
    plt.xticks(years)
    plt.legend(loc="upper left", fontsize=9, frameon=True, framealpha=0.9)
    plt.tight_layout()

    out_path = os.path.join(out_dir, "category_share_2021_2025.png")
    plt.savefig(out_path, dpi=260)
    plt.close()
    print(f"[OK] Saved stacked area figure: {out_path}")



def plot_category_leaderboards(
    trend_data: dict,
    top_n_total: int = 10,
    top_n_growth: int = 8,
    min_total_for_growth: int = 30,
    out_dir: str = "figs/category_leaderboards",
):
    """
    ‰∏§Âº†ÂõæÔºö
    1) 2021‚Äì2025 Á¥ØÁßØËÆ∫ÊñáÊï∞ Top-N ÁöÑÊñπÂêë
    2) Â¢ûÈïøÂÄçÁéá Top-N ÁöÑÈªëÈ©¨ÊñπÂêë
    """
    os.makedirs(out_dir, exist_ok=True)
    years = trend_data["years"]
    by_cat = trend_data["by_category"]
    first_idx = 0
    last_idx = len(years) - 1

    total_per_cat = {
        cat: int(sum(counts)) for cat, counts in by_cat.items() if sum(counts) > 0
    }
    if not total_per_cat:
        print("[WARN] No category data for leaderboards.")
        return

    # --- 1) total count leaderboard ---
    sorted_total = sorted(total_per_cat.items(), key=lambda kv: kv[1], reverse=True)
    top_total = sorted_total[:top_n_total]

    cats_total = [c for c, _ in top_total]
    counts_total = np.array([n for _, n in top_total], dtype=int)
    labels_total = [CATEGORY_LABELS_EN.get(c, c) for c in cats_total]

    fig1 = plt.figure(figsize=(11, 6))
    ax1 = fig1.add_subplot(111)

    cmap_total = plt.cm.get_cmap("Blues")
    colors = cmap_total(np.linspace(0.35, 0.9, len(counts_total)))
    y_pos = np.arange(len(counts_total))

    bars = ax1.barh(
        y_pos,
        counts_total,
        color=colors,
        height=0.7,
        edgecolor="#FFFFFF",
        linewidth=0.9,
    )

    ax1.set_yticks(y_pos)
    ax1.set_yticklabels(labels_total)
    ax1.set_xlabel("Number of Papers (2021‚Äì2025)")
    ax1.set_title("Top Directions by Total Paper Count", pad=14)
    ax1.xaxis.grid(True)
    ax1.yaxis.grid(False)

    for spine in ["top", "right"]:
        ax1.spines[spine].set_visible(False)

    max_count = counts_total.max()
    for bar, cnt in zip(bars, counts_total):
        x = bar.get_width()
        y = bar.get_y() + bar.get_height() / 2
        ax1.text(
            x + max_count * 0.01,
            y,
            f"{int(cnt)}",
            va="center",
            ha="left",
            fontsize=9,
            color="#333333",
        )

    fig1.tight_layout()
    out_path1 = os.path.join(out_dir, "top_total_categories.png")
    fig1.savefig(out_path1, dpi=320)
    plt.close(fig1)
    print(f"[OK] Saved total leaderboard: {out_path1}")

    # --- 2) growth leaderboard ---
    growth_info = []
    for cat, counts in by_cat.items():
        total = int(sum(counts))
        if total < min_total_for_growth:
            continue
        start = counts[first_idx]
        end = counts[last_idx]
        ratio = (end + 1) / (start + 1)
        growth_info.append((cat, ratio, start, end, total))

    if not growth_info:
        print("[WARN] No category with enough total for growth leaderboard.")
        return

    growth_sorted = sorted(growth_info, key=lambda x: x[1], reverse=True)
    top_growth = growth_sorted[:top_n_growth]

    cats_g = [c for c, _, _, _, _ in top_growth]
    ratios_g = np.array([r for _, r, _, _, _ in top_growth], dtype=float)
    labels_g = [CATEGORY_LABELS_EN.get(c, c) for c in cats_g]

    fig2 = plt.figure(figsize=(11, 6))
    ax2 = fig2.add_subplot(111)

    cmap_growth = plt.cm.get_cmap("Greens")
    colors_g = cmap_growth(np.linspace(0.35, 0.9, len(ratios_g)))
    x_pos = np.arange(len(ratios_g))

    bars = ax2.bar(
        x_pos,
        ratios_g,
        color=colors_g,
        width=0.65,
        edgecolor="#FFFFFF",
        linewidth=0.9,
    )

    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(labels_g, rotation=30, ha="right")
    ax2.set_ylabel("Growth Factor (2025 vs 2021)")
    ax2.set_title("Dark Horse Directions by Growth Factor", pad=14)
    ax2.yaxis.grid(True)
    ax2.xaxis.grid(False)

    for spine in ["top", "right"]:
        ax2.spines[spine].set_visible(False)

    for x, bar, ratio in zip(x_pos, bars, ratios_g):
        h = bar.get_height()
        ax2.text(
            x,
            h + 0.15,
            f"√ó{ratio:.1f}",
            ha="center",
            va="bottom",
            fontsize=9,
            color="#333333",
        )

    fig2.tight_layout()
    out_path2 = os.path.join(out_dir, "top_growth_categories.png")
    fig2.savefig(out_path2, dpi=320)
    plt.close(fig2)
    print(f"[OK] Saved growth leaderboard: {out_path2}")


def compute_conf_category_matrix(
    df_all: pd.DataFrame,
    focus_cats: List[str],
) :
    """
    ËøîÂõû (conferences, categories, matrix)ÔºåÂÖ∂‰∏≠ matrix[i, j] ÊòØ
    conference i Âú® category j ‰∏äÁöÑËÆ∫ÊñáÊï∞Ôºà2021‚Äì2025 Á¥ØËÆ°Ôºâ„ÄÇ
    """
    conferences = sorted(df_all["conference"].dropna().astype(str).unique().tolist())
    cats = focus_cats

    mat = np.zeros((len(conferences), len(cats)), dtype=int)

    for i, conf in enumerate(conferences):
        df_c = df_all[df_all["conference"] == conf]
        for j, cat_key in enumerate(cats):
            col_name = f"is_{cat_key}"
            if col_name in df_c.columns:
                mat[i, j] = int(df_c[col_name].sum())
            else:
                mat[i, j] = 0

    return conferences, cats, mat

def plot_conf_category_heatmap(df_all: pd.DataFrame, out_dir: str = "figs/conf_category"):
    """
    Conference √ó Direction heatmap
    - ‰ΩøÁî®Ê∏ÖÊñ∞‰∏ÄÁÇπÁöÑËìùÁªøÁ≥ªÈÖçËâ≤
    - ÂèñÊ∂àÁΩëÊ†ºÁ∫øÔºåÊï¥‰ΩìÊõ¥Âπ≤ÂáÄ
    - Âú®Ê†ºÂ≠êÈáåÊ†áÊï∞Â≠ó
    """
    os.makedirs(out_dir, exist_ok=True)

    conferences, cats, mat = compute_conf_category_matrix(df_all, FOCUS_CATS_FOR_CONF)
    if mat.size == 0:
        print("[WARN] Empty matrix for conf-category heatmap.")
        return

    cat_labels = [CATEGORY_LABELS_EN.get(c, c) for c in cats]

    fig = plt.figure(figsize=(11, 6))
    ax = fig.add_subplot(111)

    fig.patch.set_facecolor("#FFFFFF")
    ax.set_facecolor("#F6F7FB")

    # ÂÖàÂèñ‰∏Ä‰∏™ YlGnBuÔºåÁÑ∂ÂêéÊà™ÂèñÊØîËæÉ‚ÄúÊ∏ÖÊ∑°‚ÄùÁöÑ‰∏≠È´ò‰∫ÆÂå∫ÔºåÈÅøÂÖçÂ§™ÈªëÂ§™Âúü
    base_cmap = plt.cm.get_cmap("YlGnBu", 256)
    light_colors = base_cmap(np.linspace(0.25, 0.95, 256))  # 0~1 ÈáåÊà™ÊéâÊúÄÊ∑±ÁöÑÈÇ£ÊÆµ
    fresh_cmap = LinearSegmentedColormap.from_list("fresh_ylgnbu", light_colors)

    im = ax.imshow(mat, aspect="auto", cmap=fresh_cmap)

    cbar = fig.colorbar(im, ax=ax)
    cbar.set_label("Paper Count")

    ax.set_xticks(np.arange(len(cats)))
    ax.set_xticklabels(cat_labels, rotation=30, ha="right")
    ax.set_yticks(np.arange(len(conferences)))
    ax.set_yticklabels(conferences)

    ax.set_title("Conference √ó Direction Heatmap (2021‚Äì2025 total)", pad=16, fontsize=18, weight="bold")

    # ‰∏çË¶ÅËΩ¥ÁΩëÊ†ºÁ∫øÔºåÁÉ≠ÂõæËá™Â∑±Â∞±ÊòØÁΩëÊ†º
    ax.grid(False)

    # Âú®ÊØè‰∏™Ê†ºÂ≠êÈáåÂÜôÊï∞Â≠óÔºàÂ∞è‰∏ÄÁÇπÔºâ
    vmax = mat.max() if mat.max() > 0 else 1
    for i in range(mat.shape[0]):
        for j in range(mat.shape[1]):
            val = mat[i, j]
            if val == 0:
                continue
            # Ê∑±Ëâ≤ÂùóÁî®ÊµÖÂ≠óÔºåÊµÖËâ≤ÂùóÁî®Ê∑±Â≠ó
            color_text = "#FFFFFF" if val > vmax * 0.6 else "#222222"
            ax.text(
                j,
                i,
                str(val),
                ha="center",
                va="center",
                fontsize=9,
                color=color_text,
            )

    # ÂéªÊéâÁ≤óËæπÊ°Ü
    for spine in ax.spines.values():
        spine.set_visible(False)

    fig.tight_layout()
    out_path = os.path.join(out_dir, "conf_category_heatmap.png")
    fig.savefig(out_path, dpi=320)
    plt.close(fig)
    print(f"[OK] Saved conf-category heatmap: {out_path}")



def plot_conf_category_bubble(df_all: pd.DataFrame, out_dir: str = "figs/conf_category"):
    os.makedirs(out_dir, exist_ok=True)

    conferences, cats, mat = compute_conf_category_matrix(df_all, FOCUS_CATS_FOR_CONF)
    if mat.size == 0:
        print("[WARN] Empty matrix for conf-category bubble plot.")
        return

    cat_labels = [CATEGORY_LABELS_EN.get(c, c) for c in cats]
    fig = plt.figure(figsize=(11, 6))
    ax = fig.add_subplot(111)

    max_count = mat.max() if mat.max() > 0 else 1
    sizes = (mat / max_count) * 1500  # bigger for social media

    # Áî®‰∏Ä‰∏™ÊöñËâ≤Á≥ªË∞ÉËâ≤ÊùøÔºåÂ°´ÂÖÖ+Ê∑±Ëâ≤ÊèèËæπ
    cmap = plt.cm.get_cmap("Oranges")

    for i, conf in enumerate(conferences):
        for j, cat_key in enumerate(cats):
            if mat[i, j] == 0:
                continue
            color = cmap(0.35 + 0.55 * mat[i, j] / max_count)
            ax.scatter(
                j,
                i,
                s=sizes[i, j],
                alpha=0.8,
                color=color,
                edgecolors="#333333",
                linewidths=0.4,
            )

    ax.set_xticks(np.arange(len(cats)))
    ax.set_xticklabels(cat_labels, rotation=30, ha="right")
    ax.set_yticks(np.arange(len(conferences)))
    ax.set_yticklabels(conferences)

    ax.set_xlabel("Direction")
    ax.set_ylabel("Conference")
    ax.set_title("Conference √ó Direction Bubble Map (2021‚Äì2025 total)", pad=14)

    ax.grid(True, linestyle="--", alpha=0.3)
    for spine in ["top", "right"]:
        ax.spines[spine].set_visible(False)

    fig.tight_layout()
    out_path = os.path.join(out_dir, "conf_category_bubble.png")
    fig.savefig(out_path, dpi=320)
    plt.close(fig)
    print(f"[OK] Saved conf-category bubble plot: {out_path}")



def plot_conference_radar(df_all: pd.DataFrame, out_dir: str = "figs/conf_radar"):
    """
    For each conference draw a radar chart over a few key directions.
    Cleaner style: light background, no crowded radial tick labels.
    """
    os.makedirs(out_dir, exist_ok=True)

    conferences = sorted(df_all["conference"].dropna().astype(str).unique().tolist())
    cats = RADAR_CATS

    conf_cat_counts = {conf: [] for conf in conferences}
    cat_max = {c: 0 for c in cats}

    for conf in conferences:
        df_c = df_all[df_all["conference"] == conf]
        for cat_key in cats:
            col_name = f"is_{cat_key}"
            val = int(df_c[col_name].sum()) if col_name in df_c.columns else 0
            conf_cat_counts[conf].append(val)
            cat_max[cat_key] = max(cat_max[cat_key], val)

    for c in cat_max:
        if cat_max[c] == 0:
            cat_max[c] = 1

    num_vars = len(cats)
    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
    angles += angles[:1]

    labels = [CATEGORY_LABELS_EN.get(c, c) for c in cats]

    for conf in conferences:
        raw_vals = conf_cat_counts[conf]
        values_norm = [
            raw_vals[i] / cat_max[cats[i]] if cat_max[cats[i]] > 0 else 0.0
            for i in range(num_vars)
        ]
        values_norm += values_norm[:1]

        fig = plt.figure(figsize=(6, 6))
        ax = fig.add_subplot(111, polar=True)

        fig.patch.set_facecolor("#FFFFFF")
        ax.set_facecolor("#F6F7FB")

        ax.set_theta_offset(np.pi / 2)
        ax.set_theta_direction(-1)

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(labels, fontsize=9)

        # ‰∏çË¶Å‰∏ÄÂ†ÜÂçäÂæÑÂàªÂ∫¶Ê†áÁ≠æÔºåÂè™‰øùÁïôÁΩëÊ†ºÁ∫ø
        ax.set_yticks([0.25, 0.5, 0.75, 1.0])
        ax.set_yticklabels([])
        ax.set_ylim(0, 1.0)

        ax.grid(color="#D6DAE6", linestyle="--", linewidth=0.8, alpha=0.8)

        ax.plot(
            angles,
            values_norm,
            linewidth=2.2,
            linestyle="-",
            color="#FF7F50",
        )
        ax.fill(angles, values_norm, alpha=0.35, color="#FFB18A")

        ax.set_title(f"{conf}: Direction Profile (relative)", fontsize=12, pad=16)

        fig.tight_layout()
        safe_conf = conf.replace("/", "_").replace(" ", "_")
        out_path = os.path.join(out_dir, f"radar_{safe_conf}.png")
        fig.savefig(out_path, dpi=260)
        plt.close(fig)
        print(f"[OK] Saved radar for {conf}: {out_path}")


def setup_matplotlib_style():
    """
    Global matplotlib style:
    - light, slightly tinted background (not pure white)
    - modern sans-serif font
    - soft grids & legends
    """
    plt.style.use("default")

    matplotlib.rcParams.update({
        # figure
        "figure.facecolor": "#FFFFFF",
        "figure.dpi": 140,
        # axes
        "axes.facecolor": "#F6F7FB",
        "axes.edgecolor": "#E0E0E0",
        "axes.linewidth": 1.0,
        "axes.grid": True,
        "grid.color": "#E1E4EE",
        "grid.linestyle": "--",
        "grid.alpha": 0.6,
        # font
        "font.family": "DejaVu Sans",  # Âü∫Êú¨ÊâÄÊúâ Linux ÈÉΩÊúâ
        "axes.titlesize": 16,
        "axes.titleweight": "semibold",
        "axes.labelsize": 12,
        "xtick.labelsize": 10,
        "ytick.labelsize": 10,
        # legend
        "legend.frameon": True,
        "legend.framealpha": 0.95,
        "legend.facecolor": "#FFFFFF",
        "legend.edgecolor": "#E0E0E0",
    })



# ============ ‰∏ªÊµÅÁ®ã ============

def main():
    setup_matplotlib_style()
    # 1. ËØªÂèñÊâÄÊúâ CSV
    df_all = load_all_papers()

    # 2. ÊûÑÂª∫ÊØè‰∏ÄÂπ¥ÁöÑÁªüËÆ°‰ø°ÊÅØ
    year_stats = build_yearly_stats(df_all)

    # 3. ÊûÑÂª∫Â§öÂπ¥ÁöÑË∂ãÂäøÊï∞ÊçÆ
    trend_data = build_trend_data(year_stats)

    # 4. ÁîªÂõæÔºàÊú¨Âú∞ÂèØËßÜÂåñÔºâ
    plot_year_category_bars(year_stats)          # ÂéüÊù•ÁöÑÂπ¥Â∫¶ top Á±ªÂà´Êü±Áä∂ÂõæÔºàÂ¶ÇÊûú‰Ω†Ëøò‰øùÁïôÔºâ
    plot_trend_lines(trend_data)                 # ÂéüÊù•ÁöÑË∂ãÂäøÊäòÁ∫øÂõæÔºàÂèØÈÄâÔºâ
    plot_category_share_stacked_area(trend_data) # Êñ∞ÔºöÊñπÂêëÂç†ÊØîÊºîÂåñ
    plot_category_leaderboards(trend_data)       # Êñ∞ÔºöÂç∑ÁéãÊ¶úÔºàÊÄªÈáè + Â¢ûÈïøÂÄçÁéáÔºâ
    plot_conf_category_heatmap(df_all)           # Êñ∞Ôºö‰ºöËÆÆ√óÊñπÂêëÁÉ≠ÂäõÂõæ
    plot_conf_category_bubble(df_all)            # Êñ∞Ôºö‰ºöËÆÆ√óÊñπÂêëÊ∞îÊ≥°Âõæ
    plot_conference_radar(df_all)                # Êñ∞Ôºö‰ºöËÆÆ‰∫∫ËÆæÈõ∑ËææÂõæ
    save_cover_image_prompt()                    # Êñ∞ÔºöÁîüÊàêÂ∞ÅÈù¢ÊèíÁîª prompt

    # 5. Ë∞ÉÁî®Â§ßÊ®°ÂûãÂÅöÊØè‰∏ÄÂπ¥ÁöÑÊÄªÁªìÔºàÂ∞èÁ∫¢‰π¶ÊñáÊ°àÔºâ
    for year, info in year_stats.items():
        prompt = make_year_prompt(year, info)
        print(f"[LLM] Ê≠£Âú®ÁîüÊàê {year} Âπ¥ÁöÑÊÄªÁªì‚Ä¶‚Ä¶")
        content = call_llm(prompt)
        out_path = os.path.join("llm_reports", f"year_{year}_summary.md")
        save_text(out_path, content)
        print(f"[OK] ‰øùÂ≠ò {year} Âπ¥ÊÄªÁªìÂà∞ {out_path}")

    # 6. Ë∞ÉÁî®Â§ßÊ®°ÂûãÂÅö 2021‚Äì2025 Êï¥‰ΩìË∂ãÂäø + 2026 È¢ÑÊµãÔºàÂ∞èÁ∫¢‰π¶ÊñáÊ°àÔºâ
    trend_prompt = make_trend_prompt(trend_data)
    print("[LLM] Ê≠£Âú®ÁîüÊàê 2021‚Äì2025 Êï¥‰ΩìË∂ãÂäøÂàÜÊûêÂíå 2026 È¢ÑÊµã‚Ä¶‚Ä¶")
    trend_content = call_llm(trend_prompt)
    save_text("llm_reports/trend_2021_2025_and_forecast.md", trend_content)
    print("[OK] ‰øùÂ≠òÊï¥‰ΩìË∂ãÂäøÊä•ÂëäÂà∞ llm_reports/trend_2021_2025_and_forecast.md")



if __name__ == "__main__":
    main()
